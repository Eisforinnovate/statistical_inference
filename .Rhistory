}
add2(3,5)
}
add2 <- function(x,y){
x+y
}
add2(3.5)
add2(3,4)
above <-function(x,n){
use <- x>n
x[use]
}
x<-1:20
above(x,12)
columnmean<- function(y){
ne<-ncol (y)
means<- numeric(nc)
for (i in 1:nc){
means[i]<- mean(y[,1])
}
means
}
make.power<-function(n){pow<-function(x){x^n}pow}
cube <- make.power(3)
lexical scoping.R
pow
}
cube <-make.power(3)
cube <-make.power(3)
x^3
}
{
x^3
}
cube(3)
cube(3)
}
z<-10
f(3)
f(3)
z<-10
f(3)
z<-10
f(3)
f <- function(x) {
g <- function(y) {
y + z
}
z <- 4
x + g(x)
}
z<-10 f(3)
z<-10
f(3)
library(datasets)
data(iris)
?iris
librar(datasets)
library(datasets)
data(mtcars)
?mtcars
inv<- NULL
}
x = rbind(c(1, -1/4), c(-1/4, 1))
m = makeCacheMatrix(x)
makeCacheMatrix <- function(x = matrix()) {
#store cache of inverse matrix
inv<- NULL
#1. set value of the matrix
set<-function(y){
x<<-y
invCache<<-NULL
}
#2.get value for matrix
get <- function() x
setinverse <- function(inverse) inv <<- inverse
getinverse <- function() inv
list(set=set, get=get, setinverse=setinverse, getinverse=getinverse)
}
## This function computes the inverse of the special "matrix" returned by makeCacheMatrix above.
##If the inverse has already been calculated (and the matrix has not changed), then the cachesolve
##should retrieve the inverse from the cache.
cacheSolve <- function(x, ...) {
inv <- x$getinverse()
if(!is.null(inv)) {
message("just a sec, retreiving cached data.")
return(inv)
}
data <- x$get()
## Put comments here that give an overall description of what your
## functions do
clear
install.packages("swirl")
library(swirl)
swirl()
swirl()
5 + 7
x <- 5 + 7
x
y <- x-3
y
z<-c(1.1,9, 3.14)
?
z<-c(1.1,9, 3.14)
?c
z
```
xyplot(weight ~ Time | Diet, BodyWeight)
library(httr)
require(httpuv)
require(jsonlite)
```{r}
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
f <- file.path(getwd(), "ss06pid.csv")
download.file(url, f)
acs <- data.table(read.csv(f))
query1 <- sqldf("select pwgtp1 from acs where AGEP < 50")
query2 <- sqldf("select pwgtp1 from acs")  ## NO
query3 <- sqldf("select * from acs where AGEP < 50 and pwgtp1")  ## NO
query4 <- sqldf("select * from acs where AGEP < 50")  ## NO
identical(query3, query4)
```
```
```{r}
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
f <- file.path(getwd(), "ss06pid.csv")
download.file(url, f)
acs <- data.table(read.csv(f))
query1 <- sqldf("select pwgtp1 from acs where AGEP < 50")
query2 <- sqldf("select pwgtp1 from acs")  ## NO
query3 <- sqldf("select * from acs where AGEP < 50 and pwgtp1")  ## NO
query4 <- sqldf("select * from acs where AGEP < 50")  ## NO
identical(query3, query4)
```
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
f <- file.path(getwd(), "ss06pid.csv")
download.file(url, f)
acs <- data.table(read.csv(f))
acs <- data.table(read.csv("https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"))
url <- "https://d396qusza40orc.cloudfront.net/getdata%2Fdata%2Fss06pid.csv"
f <- file.path(getwd(), "ss06pid.csv")
download.file(url, f)
oauth_endpoints("github")
myapp <- oauth_app("quiz2", "ddb0d599de51ccd02f4b", secret = "6af1109f6ecf442d292425087d49bb13d9bbe9c8")
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
github_token <- oauth2.0_token(oauth_endpoints("github"), myapp)
load("data/samsungData.rda")
library(kernlab); data(spam); set.seed(333)
library(caret)
library(caret)
dim(training)
library(caret); library(kernlab); data(spam)
inTrains <-createDataPartition(y=spam$type,p=0.75,list=FALSE)
training <- spam[inTrain,]
testing <-spam[-inTrain,]
dim(training)
library(AppliedPredictiveModeling)
library(caret)
library(caret)
install.packages("forecast")Installing package(s) into ‘C:/Program Files/R/R-2.15.2/library’
install.packages("caret", dependencies = c("Depends", "Suggests"))
install.packages("AppliedPredictiveModeling")
install.packages("caret")
library(caret)
install.packages("lattice")
install.packages("lattice")
install.packages("ggplot2")
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
clear
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
pbiom(8,10,0.5,lower.tail=FALSE)
pbinom(8, 10, 0.5, lower.tail = FALSE)
ppois(40,lambda=2.5*4)
ppois(40,lambda=9*4)
ppois(40, 9 * 5) = 26%
ppois(40, 9 * 5)
n <- 100
means <- cumsum(rnorm(n))/(1:n)
means
n <- 10000
means <- cumsum(rnorm(n))/(1:n)
library (ggplot2)
g <- ggplot(data.frame(x = 1:n), y=means), aes(x=x,y=y)
round(pnorm(70,mean=80,sd=10)*100)
round(pnorm(95,mean=1100,sd=10))
round(qnorm(95,mean=1100,sd=10))
round(pnorm(14, mean=15,sd=10)*100)
round(pnorm(15, mean=15,sd=10)*100)
round(pnorm(16, mean=15,sd=10)*100)
round(pnorm(10, mean=5)*100)
round(qnorm(0.95, mean = 1100, sd = 75))
clear
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)
set.seed(3433)
library(caret)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[ inTrain,]
testing = adData[-inTrain,]
IL_str <- grep("^IL", colnames(training), value = TRUE)
preProc <- preProcess(training[, IL_str], method = "pca", thresh = 0.9)
preProc$rotation
set.seed(3433)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis, predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]
training = adData[inTrain, ]
testing = adData[-inTrain, ]
set.seed(3433)
IL_str <- grep("^IL", colnames(training), value = TRUE)
predictors_IL <- predictors[, IL_str]
df <- data.frame(diagnosis, predictors_IL)
inTrain = createDataPartition(df$diagnosis, p = 3/4)[[1]]
training = df[inTrain, ]
testing = df[-inTrain, ]
modelFit <- train(diagnosis ~ ., method = "glm", data = training)
predictions <- predict(modelFit, newdata = testing)
install.packages("rmongodb")
library("rmongodb", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
detach("package:rmongodb", unload=TRUE)
library("rmongodb", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
detach("package:rmongodb", unload=TRUE)
library("rmongodb", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
detach("package:rmongodb", unload=TRUE)
install.packages(c("caret", "class", "httr", "MASS", "Matrix", "mgcv", "RCurl", "swirl"))
load("~/.RData")
install.packages("HadoopStreaming")
install.packages("NLP")
8^2
sin(30*pi/180)
f+25
f
f<-
25
f <-
25
w<- .Last.value
w
b <-6
b<-b+5
b
install.packages("bigdata")
l <- 80 +
30
l
library(AppliedPredictiveModeling)
data(segmentationOriginal)
library(caret)
names(segmentationOrigina)
names(segmentationOriginal)
table(segmentationOriginal$Case)
library(pgmm)
library(pgmm)
install.packages("library(pgmm)")
library(pgmm)
install.packages("library(pgmm)")
install.packages("library(ElmStatLearn)")
library(ElemStatLearn)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data(vowel.train)
data(vowel.test)
set.seed(33833)
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
testError <- function(pred, data, outcome) {
sum(predict(pred, data) != outcome)/length(outcome)
}
vowelTree <- randomForest(y ~ ., data = vowel.train, prox = TRUE)
print("Test error random forest:")
setwd("~/Desktop")
setwd("~/Desktop/statistical_inference_project")
<h2><u> 2. Basical Inferential Data Analysis<u></h2>
#setting up
setwd("~/Desktop/statistical_inference_project")
lambda = 0.2 #stated above
n= 40 #stated above
numSim = 1000 #number of simulations, stated above
set.seed(25)
se <- replicate(numSim, rexp(n,lambda));p <-NULL
for (i in 1:numSim){p[i]<-mean(se[,i])}
p <- as.data.frame(p); names(p)<-"mean"
#setting up
setwd("~/Desktop/statistical_inference_project")
lambda = 0.2 #stated above
n= 40 #stated above
numSim = 1000 #number of simulations, stated above
set.seed(25)
se <- replicate(numSim, rexp(n,lambda));p <-NULL
for (i in 1:numSim){p[i]<-mean(se[,i])}
p <- as.data.frame(p); names(p)<-"mean"
<h3>3. Show that the distribution is approximately normal.</h3>
<h1>Statistical Inference Course Project</h1>
<h2><u> 1. Simulation Exercises </u></h2>
<p>The exponential distribution can be simulated in R with rexp(n, lambda) where lambda is the rate parameter. The  mean of exponential distribution is 1/lambda and the standard deviation is also also 1/lambda. Set lambda = 0.2 for all of the simulations. In this simulation, you will investigate the distribution of averages of 40 exponential(0.2)s. Note that you will need to do a thousand or so simulated averages of 40 exponentials. </p>
```{r}
#setting up
setwd("~/Desktop/statistical_inference_project")
lambda = 0.2 #stated above
n= 40 #stated above
numSim = 1000 #number of simulations, stated above
set.seed(25)
se <- replicate(numSim, rexp(n,lambda));p <-NULL
for (i in 1:numSim){p[i]<-mean(se[,i])}
p <- as.data.frame(p); names(p)<-"mean"
```
<h3>1. Show where the distribution is centered at and compare it to the theoretical center of the distribution.</h3>
```{r}
## [1] "Calculated Center = 4.99851106119285"
## [1] "Theoretical Center = 5"
```
<p>calculated center is the mean of our sample means while the theoretical mean, with lambda = 0.2, is 5.</p>
<h3>2. Show how variable it is and compare it to the theoretical variance of the distribution.</h3>
```{r}
est.sd <- sd(p$mean); est.v <- est.sd ^ 2 / nosim; the.sd <- (1/lambda * 1/sqrt(n)); the.v <- the.sd ^ 2 / nosim
p2 <- matrix(c(est.sd,est.v,the.sd,the.v),nrow = 2,ncol = 2)
rownames(p2) <- c("St. Dev. of Sample Mean","Variance of Sample Mean"); colnames(p2) <- c("Estimated","Theoretical")
p2
```
```{r}
##                         Estimated Theoretical
## St. Dev. of Sample Mean 0.7819075    0.790569
## Variance of Sample Mean 0.0006114    0.000625
```
<p>This matrix identifies the standard deviations of our sample and the theoretical values. This indicates an estimated standard error of roughly 0.78.</p>
<h3>3. Show that the distribution is approximately normal.</h3>
```{r}
require(ggplot2)
g<-ggplot(p,aes(mean))
g<-g + geom_histogram(aes(y = ..density..),fill = "orangered",binwidth = .2,color = "black")
g<-g + geom_vline(xintercept = mean(p$mean),size = .75)
g<-g + stat_function(fun=dnorm, args=list(mean=mean(p$mean), sd=est.sd),color = "darkblue", size = 1.5)
g
```
<h3> Evaluate the coverage of the confidence interval for 1/lambda: X¯±1.96Sn√. (This only needs to be done for the specific value of lambda). </h3>
```{r}
```
<h2><u> 2. Basical Inferential Data Analysis</u></h2>
<p>Now in the second portion of the class, we're going to analyze the ToothGrowth data in the R datasets package. </p>
<h3>1.Load the ToothGrowth data and perform some basic exploratory data analyses</h3>
<h3>2.Provide a basic summary of the data.</h3>
<h3>3.Use confidence intervals and hypothesis tests to compare tooth growth by supp and dose. (Use the techniques from class even if there's other approaches worth considering)</h3>
<h3>4.State your conclusions and the assumptions needed for your conclusions.</h3>
install.packages("gridExtra")
library("gridExtra", lib.loc="/Library/Frameworks/R.framework/Versions/3.1/Resources/library")
